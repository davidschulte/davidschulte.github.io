<!DOCTYPE HTML>
<!--
	Massively by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Generic Page - Massively by HTML5 UP</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="../assets/css/main.css" />
		<noscript><link rel="stylesheet" href="../assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header">
						<a href="../index.html" class="logo">Massively</a>
					</header>

				<!-- Nav -->
					<nav id="nav">
						<ul class="links">
							<li><a href="../index.html">This is Massively</a></li>
							<li class="active"><a href="../generic.html">Generic Page</a></li>
							<li><a href="../elements.html">Elements Reference</a></li>
						</ul>
						<ul class="icons">
							<li><a href="#" class="icon brands fa-twitter"><span class="label">Twitter</span></a></li>
							<li><a href="#" class="icon brands fa-facebook-f"><span class="label">Facebook</span></a></li>
							<li><a href="#" class="icon brands fa-instagram"><span class="label">Instagram</span></a></li>
							<li><a href="#" class="icon brands fa-github"><span class="label">GitHub</span></a></li>
						</ul>
					</nav>

				<!-- Main -->
					<div id="main">

						<!-- Post -->
							<section class="post">
								<header class="major">
									<span class="date">April 25, 2017</span>
									<h1>Reinforcement Learning<br />
										for 3-player<br />
										Chinese Checkers</h1>
									<p>AlphaZero is impressive. But what happens when we apply its logic to a multiplayer game?</p>
								</header>
								<div class="image main"><img src="images/alphathesis.png" alt="" /></div>
								<h2>Introduction</h2>
								<h3>Motivation</h3>
								<p>Like for many aspiring Machine Learning enthusiasts, the development of AlphaZero could be described as the one event that made me sure, this is the right field for me. I was curious how a program can learn such sophisticated tactics in several domain just by playing against itself.
									I researched about the design of its algorithm and fascinated by it, I decided to dedicate my Bachelor's thesis to experimenting with it. <br \>
									The program is designed for learning a strategy for two-player zero-sum games. But what would happen, if we break this assumption?
								</p>
								<h3>The game</h3>
								<p>As application I chose a game I know since my childhood: the multiplayer board game Sternhalma (engl. Chinese Checkers). The goal of each player is to maneuver their figures from their designated starting zone across the board into their end zone before the others manage to do so. Players take turns and can try to advance their pieces but also block those of their opponents. The tactical core of the game is its jumping role which enables long sequences of movements across the board.</p>
								<h2>How it works</h2>
								<h3>Monte-Carlo tree search and Neural Net</h3>
								AlphaZero is the successor of AlphaGo and AlphaGo Zero, all developed by the research company Deep Mind.
								The program trains an agent that can surpass previous human as well as algorithmic agents without any prior knowledge about its domains, except for their rules. The domain that was used for its showcase is the board game Go.
								<p>The core of AlphaZero is the combination of Convolutional Neural Network and a game tree which is traversed using a Monte-Carlo tree search.
									The game tree of complex games like Go grows so quickly that even algorithmicly it becomes unfeasible to simulate all possible positions after even a few turns from the current game state. But why check every possible combination of plays when it is obvious that some of them are just bad? If we only knew which branches of the tree are even worth looking into. That is where Deep Learning comes into play. Given a game state, a neural network predicts how likely each player is to win, and which branches of the tree are worth looking into. This is incorporated in a Monte-Carlo tree search, which enables the program to navigate deeper into the game tree and find long-term strategies.
								</p>
								<p>If you are interested in finding out more about the concept, visit the <a href="https://www.deepmind.com/blog/alphazero-shedding-new-light-on-chess-shogi-and-go">offical page by Deepmind</a>. <br \>
								Another excellent resource is <a href="https://web.stanford.edu/~surag/posts/alphazero.html">this explaination by Surag Nair</a>. The corresponding Github repository of a simplified version of AlphaZero was used as a basis for my project.</p>
								<h3>Self-Play</h3>
								The agent's training is carried out through self-play. We let the current version play against identical duplicates of itself. This results in played games, in which some moves lead to a better position while some moves lead to a loss. We let the agents play a fixed number of games and zip each played move together with the score it resulted in in the end and feed this as training data into the neural network. Thus, the network learns to favor moves that lead to a victory. After training a new version of the agent, we let it play against the current version and if it outperforms the status quo, it will be the new standard.
								<div class="col-4"><span class="image fit"><img src="images/flowchart.png" alt="" /></span></div>
								<h2>Results</h2>
								<h3>Metrics</h3>
								One of the metric used to evaluate the agent is letting it play against a greedy agent. A greedy agent makes one of the moves that advance his pieces forward the most but does not plan ahead.
								Since we are evaluating in a 3-player game, we display both scenarios: 2 Alphas vs. 1 Greedy Agent and 1 Alpha vs. 2 Greedy Agents.
								<div class="col-4"><span class="image fit"><img src="images/mcts_vs_greedy.png" alt="" /></span></div>
								<h3>Tactics</h3>
								Since the board is relatively small, the agents try to block their opponents during the middle game and then try to race their pieces into the end zone in the last minute.
								<div class="col-4"><span class="image fit"><img src="images/block2.png" alt="" /></span></div>
								<h2>Difficulties</h2>
								<h3>Game Loops</h3>
								<p>A game loop is a sequence of moves by players that ends in the same game state it started in. Board games that are played competitively usually have rules that prevent these loops. For example, even in the game of chess where one could imagine such a sequence, the <a href="https://en.wikipedia.org/wiki/Fifty-move_rule">Fifty-move rule</a> prevents game loops and endless play.</p>
								<p>Sternhalma traditionally does not have rule to prevent game loops. This especially problematic because a loop in the game destroys the one-directional structure of its game tree, through which we have to navigate. To fix this problem, I added such a rule to the game.
									However, the agents sometimes also found a workaround around that and exploited quasi-loops. Sequences that are not identical but lead to the same result. I added a second rule that terminates the game after a specified number of turns.
									In the graphic below, you can observe how the yellow agent tries to exploit a quasi-loop. Green has already won the first place and Red only needs to bring one more figure into its end zone. Since Green realizes that it will lose the game, it decides to block Red in its end zone and drag the game on hopelessly instead of giving up and losing the game.
								</p>
								<div class="col-4"><span class="image fit"><img src="images/loop.png" alt="" /></span></div>
								<h3>Exploration vs. Exploitation</h3>
								<p>Another difficulty is the right parameter that balances between Exploration and Exploitation. This an important tradeoff when learning a new task. In our case exploration means trying out new tactics that do not seem promising at first glance in the hope of finding a hidden ingenious move. Exploitation corresponds to focussing on moves that look promising and predict which of them will have been the best many moves later in the game. In our game tree those concepts are analoguous to breadth-first and depth-first search. Our Monte-Carlo tree search contains a parameter that controls the balancing of both concepts. To find the optimal value for this paramater is not trivial and greatly depends on the nature of the game. In this project, I experimented with different values, but it is difficult to determine its optimum</p>
								<h2>Learnings and Outlook</h2>
								At the time I was a total beginner in Python and the field of AI. Therefore, this turned out to be a very challenging project. Nonetheless, it was a success resulting in a strong agent after 20 episodes of training. The bachelor's thesis was well received and was graded with an 1.0 (A+). It contains more details and explanations about the project. You can read it <a href="documents/Thesis.pdf">here</a>. <br />
								Looking back now at it, I see a lot of things I would do different now. This mainly concerns the implementation and structure of the code. On the other hand, one can see this as measure of how much I learnt since. <br />
								The project also contains an implementation, in which the user can play against one or several trained agents. In the future, I would like to find a way to host this game online, as it can be a lot more impressive to lose against the program than seeing its performance on charts. 
							</section>
					</div>

				<!-- Footer -->
					<footer id="footer">
						<section>
							<form method="post" action="#">
								<div class="fields">
									<div class="field">
										<label for="name">Name</label>
										<input type="text" name="name" id="name" />
									</div>
									<div class="field">
										<label for="email">Email</label>
										<input type="text" name="email" id="email" />
									</div>
									<div class="field">
										<label for="message">Message</label>
										<textarea name="message" id="message" rows="3"></textarea>
									</div>
								</div>
								<ul class="actions">
									<li><input type="submit" value="Send Message" /></li>
								</ul>
							</form>
						</section>
						<section class="split contact">
							<section class="alt">
								<h3>Address</h3>
								<p>1234 Somewhere Road #87257<br />
								Nashville, TN 00000-0000</p>
							</section>
							<section>
								<h3>Phone</h3>
								<p><a href="#">(000) 000-0000</a></p>
							</section>
							<section>
								<h3>Email</h3>
								<p><a href="mailto:davidsiriusschulte@gmail.com">davidsiriusschulte@gmail.com</a></p>
							</section>
							<section>
								<h3>Social</h3>
								<ul class="icons alt">
									<!-- <li><a href="#" class="icon brands alt fa-twitter"><span class="label">Twitter</span>@davidschulte</a> </li>
									<li><a href="#" class="icon brands alt fa-facebook-f"><span class="label">Facebook</span></a></li>
									<li><a href="#" class="icon brands alt fa-instagram"><span class="label">Instagram</span></a></li> -->
									<li><a href="https://github.com/davidschulte" class="icon brands alt fa-github"><span class="label">GitHub</span>/davidschulte</a></li>
								</ul>
							</section>
						</section>
					</footer>

				<!-- Copyright -->
					<div id="copyright">
						<ul><li>&copy; Untitled</li><li>Design: <a href="https://html5up.net">HTML5 UP</a></li></ul>
					</div>

			</div>

		<!-- Scripts -->
			<script src="../assets/js/jquery.min.js"></script>
			<script src="../assets/js/jquery.scrollex.min.js"></script>
			<script src="../assets/js/jquery.scrolly.min.js"></script>
			<script src="../assets/js/browser.min.js"></script>
			<script src="../assets/js/breakpoints.min.js"></script>
			<script src="../assets/js/util.js"></script>
			<script src="../assets/js/main.js"></script>

	</body>
</html>